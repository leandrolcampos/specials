{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright 2023 The Specials Authors. Licensed under the Apache License, Version 2.0 (the \"License\").*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Some of the code in this file is adapted from:_\n",
    "\n",
    "_modularml/mojo_<br>\n",
    "_Copyright (c) 2023, Modular Inc._<br>\n",
    "_Licensed under the Apache License v2.0 with LLVM Exceptions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The log-beta function in Specials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the implementation of the **log-beta function** in Specials and assesses its quality in terms of both numerical accuracy and stability, as well as computational performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-beta function is the natural logarithm of the [beta function](https://en.wikipedia.org/wiki/Beta_function) $\\text{B}$, which is a special function defined by the integral\n",
    "\n",
    "$$\\text{B}(x,y) = \\int_{0}^{1} t^{x-1}(1-t)^{y-1} \\,dt$$\n",
    "\n",
    "for real numbers $x, y > 0$. The beta function is renowned for its applications in mathematics, physics, probability, and statistics. Recently, there has been a growing interest in the AI applications of this function for building deep probabilistic models with random variables following the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) or the [Student's _t_-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution). In these cases, the beta function is used, for example, to define [likelihood functions](https://en.wikipedia.org/wiki/Likelihood_function) or to compute [implicit reparameterization gradients](https://arxiv.org/abs/1805.08498)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key property of the beta function is its close relationship to the [gamma function](https://en.wikipedia.org/wiki/Gamma_function) $\\Gamma$:\n",
    "\n",
    "$$\\text{B}(x,y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)} .$$\n",
    "\n",
    "This relationship allows us to express the log-beta in terms of the log-gamma function:\n",
    "\n",
    "$$\\ln \\text{B}(x,y) = \\ln \\Gamma(x) + \\ln \\Gamma(y) - \\ln \\Gamma(x+y) .$$\n",
    "\n",
    "For numerical reasons and because the beta function grows very rapidly for moderately large arguments, one usually works with the latter equation instead of directly evaluating the beta function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call **naive solution** of the log-beta function an implementation based solely on this latter equation. This solution presents accuracy loss when at least one of its arguments takes a large value. The reason is [catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation) in the numerical evaluation of the difference\n",
    "\n",
    "$$\\ln \\Gamma(y) - \\ln \\Gamma(x+y) .$$\n",
    "\n",
    "Assume, without loss of generality (since log-beta is symmetric), that the condition $x \\leq y$ holds. When $y$ is large compared to $x$, this difference tends to the difference between two almost identical numbers, which extinguishes most significant digits [[2](#machler)]. It's important to note that although $\\ln \\Gamma(z)$ grows less rapidly than $\\Gamma(z)$ as $z$ increases, $\\ln \\Gamma(z)$ still behaves proportionally to $z\\ln(z) - z$ asymptotically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specials and other scientific packages avoid this numerical cancellation by explicitly decomposing $\\ln \\Gamma(z)$, for sufficiently large $z$, into the approximation\n",
    "\n",
    "$$\\ln \\Gamma(z) \\approx (z - 0.5)\\ln z - z + 0.5 \\ln (2 \\pi)$$\n",
    "\n",
    "and a correction term (which we'll call **log-gamma correction**), and then cancelling the large terms from the\n",
    "approximation analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, let's install, if necessary, all the Python packages that will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from importlib.util import find_spec\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "fix = \"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "To fix this, follow the steps in the link below:\n",
    "    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def install_if_missing(name: str):\n",
    "    if find_spec(name.replace(\"-\", \"_\")):\n",
    "        return\n",
    "\n",
    "    print(f\"The package `{name}` was not found. We will install it...\")\n",
    "    try:\n",
    "        if shutil.which(\"python3\"): python = \"python3\"\n",
    "        elif shutil.which(\"python\"): python = \"python\"\n",
    "        else:\n",
    "            raise RuntimeError(\"Python is not on `PATH`. \" + fix)\n",
    "        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n",
    "    except:\n",
    "        raise ImportError(f\"The package `{name}` was not found. \" + fix)\n",
    "\n",
    "install_if_missing(\"mpmath\")\n",
    "install_if_missing(\"numpy\")\n",
    "install_if_missing(\"scipy\")\n",
    "install_if_missing(\"tabulate\")\n",
    "install_if_missing(\"tensorflow\")\n",
    "install_if_missing(\"tensorflow-probability\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Approximating the log-gamma correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the log-gamma correction, Specials provides the function `lgamma_correction`. Since there is no analytical solution for this correction, the function evaluates it using a [Chebyshev approximation](https://mathworld.wolfram.com/ChebyshevApproximationFormula.html). We will introduce the Chebyshev approximation later in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `lgamma_correction` is defined for $x \\geq 8$, where $x$ is a single or double precision floating-point argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 0.010411262512207031, 0.0092554632574319839, 0.0083305658772587776]\n"
     ]
    }
   ],
   "source": [
    "import specials\n",
    "\n",
    "let x = SIMD[DType.float32, 4](7.0, 8.0, 9.0, 10.0)\n",
    "let res = specials.lgamma_correction(x)\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `lgamma_correction` returns `nan` when $x < 8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare with the values returned by the Specials function, we'll call the corresponding function from the [TensorFlow Probability](https://www.tensorflow.org/probability) package using the same input values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01041127 0.00925546 0.00833056]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "\n",
    "x = np.array([8.0, 9.0, 10.0], dtype=np.float32)\n",
    "res = tfp.math.log_gamma_correction(x)\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's introduce the Chebyshev approximation with an illustrative example. Let $T_k(x)$ be the Chebyshev polynomial of the first kind with degree $k$, $k = 0, 1, \\dots$, and let $f$ be a real function defined in the interval $[-1, 1]$ such that $f(x) = \\exp(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $N = 15$, we use the Python packages [`mpmath`](https://mpmath.org/) and [NumPy](https://numpy.org/) to find the coefficients $c_k, k = 0, \\dots, N - 1$, such that the approximation\n",
    "\n",
    "$$f(x) \\approx \\sum_{k=0}^{N-1} c_kT_k(x),$$\n",
    "\n",
    "known as the Chebyshev approximation, is exact for $x$ equal to all $k$ zeros of the $T_k$ polynomial. All zeros of $T_k$ are located in the interval $[-1, 1]$, where the function $f(x)$ is defined by construction. The estimated maximum absolute error for this approximation is about $5\\times10^{-17}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Specials to generate a Chebyshev series with the coefficients obtained earlier. The class representing a Chebyshev series in Specials is designed to support various manipulations in the Mojo parameter space during compile time. Moreover, this class performs bounds and type checks during compilation, eliminating operations and preventing errors at runtime. For example, it is not possible to compile code that provides a quantity of coefficients different from the number of terms in the series or attempts to access a nonexistent coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p.num_terms = 15\n",
      "p.degree() = 14\n",
      "p.get[0]() = [1.2660658359527588, 1.2660658359527588, 1.2660658359527588, 1.2660658359527588]\n"
     ]
    }
   ],
   "source": [
    "from specials.polynomial import Chebyshev, chebyshev_eval, chebyshev_init\n",
    "\n",
    "# This Chebyshev series has 15 terms (it's a polynomial with degree 14).\n",
    "alias p = Chebyshev[15, DType.float32, simd_width=4].from_coefficients[\n",
    "    1.266065877752008335598244625215e-00,\n",
    "    1.130318207984970054415392055220e-00,\n",
    "    2.714953395340765623657051399900e-01,\n",
    "    4.433684984866380495257149525980e-02,\n",
    "    5.474240442093732650276168431186e-03,\n",
    "    5.429263119139437503621478103037e-04,\n",
    "    4.497732295429514665469032791685e-05,\n",
    "    3.198436462401990505863863657617e-06,\n",
    "    1.992124806672795725956775710929e-07,\n",
    "    1.103677172551734430729047687106e-08,\n",
    "    5.505896079673739316799448763965e-10,\n",
    "    2.497956616981807165974951489258e-11,\n",
    "    1.039152229471141123219881783361e-12,\n",
    "    3.991259006494906568717874813903e-14,\n",
    "    1.422277830768448851326979168181e-15,\n",
    "]()\n",
    "\n",
    "print(\"p.num_terms =\", p.num_terms)\n",
    "print(\"p.degree() =\", p.degree())\n",
    "print(\"p.get[0]() =\", p.get[0]())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key property of the Chebyshev approximation is that it can be truncated to a polynomial with lower degree $m \\ll N-1$ that is very close to the [minimax polynomial](https://mathworld.wolfram.com/MinimaxPolynomial.html): among all polynomials of the same degree, the minimax polynomial is the one that has the smallest maximum deviation from the true function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Specials, we can find the smallest value $m$ such that the maximum deviation between the original approximating polynomial of degree $N-1$ and its truncated version with degree $m$ is upper bounded by a requested accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At compile time, we can then truncate the original Chebyshev series and later utilize the truncated version to approximate the true function $f(x)$, thereby saving operations when evaluating the approximation for various $x$ values at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requested_accuracy = 5.9604644775390625e-08\n",
      "p_truncated.num_terms = 9\n",
      "p_truncated.degree() = 8\n"
     ]
    }
   ],
   "source": [
    "from specials._internal.limits import FloatLimits\n",
    "\n",
    "alias requested_accuracy = FloatLimits[DType.float32].epsneg\n",
    "print(\"requested_accuracy =\", requested_accuracy)\n",
    "\n",
    "alias num_terms = chebyshev_init[\n",
    "    p.num_terms, p.dtype, p.simd_width, p, requested_accuracy\n",
    "]()\n",
    "\n",
    "alias p_truncated = p.truncate[num_terms]()\n",
    "print(\"p_truncated.num_terms =\", p_truncated.num_terms)\n",
    "print(\"p_truncated.degree() =\", p_truncated.degree())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the truncated approximating polynomial returns values that apparently do not differ from those we can obtain by directly calling the standard library function `math.exp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36787945032119751, 1.0, 1.6487212181091309, 2.7182817459106445]\n",
      "[0.36787945032119751, 1.0, 1.6487212181091309, 2.7182817459106445]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "let x = SIMD[DType.float32, 4](-1.0, 0.0, 0.5, 1.0)\n",
    "let res = chebyshev_eval(p_truncated, x)\n",
    "\n",
    "print(res)\n",
    "print(math.exp(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating the log-beta function implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the value of the log-beta function for real numbers $x, y$, Specials provides the `lbeta` procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1584847490202885, -2.0794415416798362, -22.572893813393978, 23.025850927580152]\n"
     ]
    }
   ],
   "source": [
    "let x = SIMD[DType.float64, 4](0.1, 8.0, 30.0, 1e10)\n",
    "let y = SIMD[DType.float64, 4](3.0, 1.0, 10.0, 1e-10)\n",
    "let res = specials.lbeta(x, y)\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the arguments of `lbeta` are not positive and finite, the returned value is the same as that of the corresponding procedure in the R language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, -inf, nan, nan, inf, -inf, -inf]\n"
     ]
    }
   ],
   "source": [
    "let inf = math.limit.inf[DType.float64]()\n",
    "let nan = math.nan[DType.float64]()\n",
    "let x = SIMD[DType.float64, 8](-1.0, -5.0, inf, 1.0, nan, 0.0, 1.0, inf)\n",
    "let y = SIMD[DType.float64, 8](10.0, -2.5, inf, nan, 1.0, 0.0, inf, 1.0)\n",
    "let res = specials.lbeta(x, y)\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four functions, defined for internal use in the Specials package, play fundamental roles in this experiment, and for this reason, we present them here:\n",
    "* `elementwise`: Applies a mathematical operator to one or more tensors element-wise, in a vectorized and potentially parallelized manner. Currently, it only supports binary operators: tensor-tensor and tensor-scalar.\n",
    "* `random_uniform`: Creates a new tensor whose elements are uniformly sampled from the closed interval `[min_value, max_value]`.\n",
    "* `run_benchmark`: Benchmarks the mathematical operator passed as a parameter using the `elementwise` function.\n",
    "* `tensor_to_numpy_array`: Converts a Mojo tensor into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specials._internal.tensor import (\n",
    "    elementwise,\n",
    "    random_uniform,\n",
    "    run_benchmark,\n",
    "    tensor_to_numpy_array,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate the first two of these four functions in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[2.3504969739512394, 3.1375148487639053]], dtype=float64, shape=2)\n"
     ]
    }
   ],
   "source": [
    "let x = random_uniform[DType.float64](1.0, 2.0, 2)\n",
    "let y = random_uniform[DType.float64](1.0, 2.0, 2)\n",
    "let res = elementwise[math.add](x, y)\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we evaluate `specials.lbeta` in terms of numerical **accuracy** (closeness between the result computed by `lbeta` and the true expected result) and **stability** (the ability of the procedure to produce consistent and reliable results in various input scenarios). Additionally, we examine its computational performance, measured by the execution time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Experimental Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess whether the implementation of the log-beta function in Specials produces consistent and reliable results in various input scenarios, we uniformly sampled 50,000 values for its arguments from 5 (five) intervals of the form $(0, b]$, referred to as _domains_, where $b = 10^1, \\dots, 10^5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we exclusively worked with double-precision floating-point arguments (`float64`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.static_tuple import StaticTuple\n",
    "\n",
    "alias dtype = DType.float64\n",
    "\n",
    "let min_value = FloatLimits[dtype].eps\n",
    "let max_values = StaticTuple[5, FloatLiteral](10.0, 100.0, 1_000.0, 10_000.0, 100_000.0)\n",
    "let num_samples = 50_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the accuracy of a log-beta function implementation, we use the _relative error_. Let $\\hat{x}$ be an approximation of the real number $x$. The relative error $E_{\\text{rel}}(\\hat{x})$ is given by:\n",
    "\n",
    "$$E_{\\text{rel}}(\\hat{x}) = \\frac{|x - \\hat{x}|}{|x|}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the arguments, the exact but unknown value of the log-beta function, represented here by the real number $x$, is computed with high precision using the Python library [`mpmath`](https://mpmath.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare different implementations in terms of accuracy, we calculate the maximum and mean values of the relative error for each combination of implementation and domain. Lower values indicate higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quantifying computational performance, we measure the _execution time_: in Mojo, using the `benchmark` module, and in Python, by defining a function based on the `timeit` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare different implementations in terms of computational performance, we calculate the mean execution time for each combination of implementation and domain. Smaller results indicate better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python import Python\n",
    "from python.object import PythonObject\n",
    "\n",
    "from specials._internal.tensor import BinaryOperator\n",
    "\n",
    "Python.add_to_path(\".\")\n",
    "\n",
    "\n",
    "fn solution_report[\n",
    "    solution_name: StringLiteral,\n",
    "    func: BinaryOperator,\n",
    "    dtype: DType,\n",
    "    simd_width: Int = simdwidthof[dtype](),\n",
    "](x: Tensor[dtype], y: Tensor[dtype], truth: PythonObject) raises -> PythonObject:\n",
    "    \"\"\"Computes the evaluation metrics for a given numerical solution in Mojo.\"\"\"\n",
    "    let builtins = Python.import_module(\"builtins\")\n",
    "    let np = Python.import_module(\"numpy\")\n",
    "    let numerics_testing = Python.import_module(\"specials._internal.numerics_testing\")\n",
    "\n",
    "    let result = elementwise[func](x, y)\n",
    "    let msecs = run_benchmark[func](x, y).mean[\"ms\"]()\n",
    "    let relerr = numerics_testing.py_relative_error(\n",
    "        tensor_to_numpy_array(result), truth\n",
    "    )\n",
    "\n",
    "    let report = builtins.list()\n",
    "    _ = report.append(solution_name)\n",
    "    _ = report.append(np.max(relerr))\n",
    "    _ = report.append(np.mean(relerr))\n",
    "    _ = report.append(msecs)\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from timeit import timeit\n",
    "\n",
    "import mpmath as mp\n",
    "\n",
    "from specials._internal import numerics_testing\n",
    "\n",
    "def py_mpmath_lbeta(x, y):\n",
    "    \"\"\"Computes the log-beta function using `mpmath`.\"\"\"\n",
    "    def _mp_lbeta_impl(a, b):\n",
    "        a = mp.mpf(a)\n",
    "        b = mp.mpf(b)\n",
    "        with mp.workdps(30):\n",
    "            res = mp.log(mp.beta(a, b))\n",
    "        return res\n",
    "\n",
    "    dtype = np.result_type(x, y)\n",
    "    return np.frompyfunc(_mp_lbeta_impl, 2, 1)(x, y).astype(dtype)\n",
    "\n",
    "\n",
    "def py_benchmark(func, *args):\n",
    "    \"\"\"Computes the average execution time of a Python function.\"\"\"\n",
    "    # Warmup phase\n",
    "    _ = timeit(lambda: func(*args), number=2)\n",
    "\n",
    "    msecs = 1000 * timeit(lambda: func(*args), number=100) / 100\n",
    "    return msecs\n",
    "\n",
    "\n",
    "def py_solution_report(solution_name, func, x_arr, y_arr, truth):\n",
    "    \"\"\"Computes the evaluation metrics for a given numerical solution in Python.\"\"\"\n",
    "    result = func(x_arr, y_arr)\n",
    "    msecs = py_benchmark(func, x_arr, y_arr)\n",
    "    relerr = numerics_testing.py_relative_error(result, truth)\n",
    "\n",
    "    return [solution_name, np.max(relerr), np.mean(relerr), msecs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the log-beta function implementation in Specials with the following alternative solutions:\n",
    "- **Naive**: It is an implementation based solely on the standard library function `math.lgamma`.\n",
    "- **SciPy**: [SciPy](https://scipy.org/) is considered one of the leading Python libraries for scientific computing. It provides the `betaln` function, which is a [NumPy UFunc](https://numpy.org/devdocs/user/basics.ufuncs.html#ufuncs-basics) that wraps a C implementation available in the Cephes Mathematical Library [[link](https://netlib.org/cephes/)].\n",
    "- **TFP[numpy]**: [TensorFlow Probability](https://www.tensorflow.org/probability) is a Python library in the TensorFlow ecosystem focused on deep probabilistic models. It provides an implementation for the log-beta function, `lbeta`, that is based on the method proposed in [[1](#didonato1992)]. In the library's `numpy` substrate, this function is defined using NumPy operators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn naive_lbeta[\n",
    "    dtype: DType, simd_width: Int\n",
    "](x: SIMD[dtype, simd_width], y: SIMD[dtype, simd_width]) -> SIMD[dtype, simd_width]:\n",
    "    \"\"\"Computes the log-beta function using the naive implementation.\"\"\"\n",
    "    return math.lgamma(x) + math.lgamma(y) - math.lgamma(x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import scipy\n",
    "\n",
    "\n",
    "py_scipy_lbeta = scipy.special.betaln\n",
    "py_tfp_lbeta = tfp.math.lbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the experiment and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from tabulate import tabulate, SEPARATING_LINE\n",
    "\n",
    "\n",
    "def py_print_table(data, domain_names, num_solutions):\n",
    "    \"\"\"Prints the evaluation metrics for all numerical solutions.\"\"\"\n",
    "    headers = [\n",
    "        \"\\nDomain\",\n",
    "        \"\\nSolution\",\n",
    "        \"Maximum\\nRelative Error\",\n",
    "        \"Mean\\nRelative Error\",\n",
    "        \"Mean Execution Time\\n(in milliseconds)\",\n",
    "    ]\n",
    "\n",
    "    # Insert domain names\n",
    "    current_domain = 0\n",
    "    for i, report in enumerate(data):\n",
    "        if i % num_solutions == 0:\n",
    "            data[i].insert(0, domain_names[current_domain])\n",
    "            current_domain += 1\n",
    "        else:\n",
    "            data[i].insert(0, \"\")\n",
    "\n",
    "    # Insert horizontal lines between domains\n",
    "    for index in range(num_solutions, len(data) + num_solutions, num_solutions + 1):\n",
    "        data.insert(index, SEPARATING_LINE)\n",
    "\n",
    "    floatfmt = (\".2e\", \".2e\", \".2e\", \".2e\", \".3f\")\n",
    "    table = tabulate(data, headers, tablefmt=\"simple\", floatfmt=floatfmt)\n",
    "\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the experiment. This may take a while...\n",
      "\n",
      "                               Maximum              Mean    Mean Execution Time\n",
      "Domain    Solution      Relative Error    Relative Error      (in milliseconds)\n",
      "--------  ----------  ----------------  ----------------  ---------------------\n",
      "0,10      Specials            6.65e-12          7.97e-16                  3.297\n",
      "          Naive               8.02e-12          1.25e-15                  2.458\n",
      "          SciPy               2.13e-11          1.50e-15                  4.651\n",
      "          TFP[numpy]          1.43e-11          9.85e-16                 15.169\n",
      "--------  ----------  ----------------  ----------------  ---------------------\n",
      "0,100     Specials            4.75e-14          5.70e-17                  2.594\n",
      "          Naive               1.42e-12          1.32e-15                  2.279\n",
      "          SciPy               1.60e-12          6.61e-16                  7.166\n",
      "          TFP[numpy]          6.63e-14          7.31e-17                 12.731\n",
      "--------  ----------  ----------------  ----------------  ---------------------\n",
      "0,1000    Specials            2.11e-13          5.04e-17                  2.454\n",
      "          Naive               1.78e-10          7.41e-15                  2.262\n",
      "          SciPy               1.78e-10          7.80e-15                  3.130\n",
      "          TFP[numpy]          1.80e-13          6.35e-17                 13.066\n",
      "--------  ----------  ----------------  ----------------  ---------------------\n",
      "0,10000   Specials            1.58e-15          4.55e-17                  2.638\n",
      "          Naive               1.18e-10          4.80e-15                  2.357\n",
      "          SciPy               1.18e-10          5.16e-15                  2.818\n",
      "          TFP[numpy]          1.75e-15          6.01e-17                 12.082\n",
      "--------  ----------  ----------------  ----------------  ---------------------\n",
      "0,100000  Specials            3.22e-16          4.54e-17                  2.500\n",
      "          Naive               4.30e-11          4.48e-15                  2.262\n",
      "          SciPy               2.73e-11          4.41e-15                  2.693\n",
      "          TFP[numpy]          3.86e-16          5.87e-17                 12.028\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "let builtins = Python.import_module(\"builtins\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "let domain_name = String(\"0,\")\n",
    "domain_names = builtins.list()\n",
    "\n",
    "var data = builtins.list()\n",
    "\n",
    "print(\"Running the experiment. This may take a while...\\n\")\n",
    "\n",
    "for i in range(max_values.__len__()):\n",
    "    let max_value = max_values[i]\n",
    "\n",
    "    domain_names.append(domain_name + max_value.to_int())\n",
    "    let a = random_uniform[dtype](min_value, max_value, num_samples)\n",
    "    let b = random_uniform[dtype](min_value, max_value, num_samples)\n",
    "    let a_arr = tensor_to_numpy_array(a)\n",
    "    let b_arr = tensor_to_numpy_array(b)\n",
    "\n",
    "    # mpmath\n",
    "    let truth = py_mpmath_lbeta(a_arr, b_arr)\n",
    "\n",
    "    # Specials\n",
    "    let specials_report = solution_report[\"Specials\", specials.lbeta, dtype](\n",
    "        a, b, truth\n",
    "    )\n",
    "    data.append(specials_report)\n",
    "\n",
    "    # Naive\n",
    "    let naive_report = solution_report[\"Naive\", naive_lbeta, dtype](a, b, truth)\n",
    "    data.append(naive_report)\n",
    "\n",
    "    # SciPy\n",
    "    let scipy_report = py_solution_report(\"SciPy\", py_scipy_lbeta, a_arr, b_arr, truth)\n",
    "    data.append(scipy_report)\n",
    "\n",
    "    # TensorFlow Probability, NumPy substrate\n",
    "    let tfp_report = py_solution_report(\"TFP[numpy]\", py_tfp_lbeta, a_arr, b_arr, truth)\n",
    "    data.append(tfp_report)\n",
    "\n",
    "py_print_table(data, domain_names, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the maximum relative error, the Specials implementation shows the best results, except in the domain (0, 1000], where the TensorFlow Probability implementation demonstrates higher accuracy. As for the mean relative error, Specials outperforms the competitors unanimously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the mean execution time, the naive, Specials, and SciPy implementations are in the same order of magnitude. Overall, the naive implementation exhibits the best performance, followed by Specials, and then SciPy. In all evaluated domains, the TensorFlow Probability implementation has shown to be an order of magnitude inferior to the other solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a id=\"didonato1992\">1</a>]\n",
    "Didonato, A. R., & Morris Jr, A. H. (1992). Algorithm 708: Significant digit computation of the incomplete beta function ratios. _ACM Transactions on Mathematical Software (TOMS)_, 18(3), 360-373. [[Link](https://dl.acm.org/doi/10.1145/131766.131776)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<a id=\"machler\">2</a>]\n",
    "MÃ¤chler, M. Computing the Beta Function for Large Arguments. [[Link](https://cran.r-project.org/web/packages/DPQ/vignettes/comp-beta.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: System information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, information about the system used to run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modular 0.2.0 (355ea9c0)\n",
      "mojo 0.5.0 (6e50a738)\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "subprocess.run([\"modular\", \"-v\"])\n",
    "subprocess.run([\"mojo\", \"-v\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information\n",
      "    OS          : linux\n",
      "    CPU         : haswell\n",
      "    Arch        : x86_64-unknown-linux-gnu\n",
      "    Num Cores   : 4\n",
      "    CPU Features: sse4 avx avx2\n"
     ]
    }
   ],
   "source": [
    "from runtime.llcl import num_cores\n",
    "from sys.info import (\n",
    "    os_is_linux,\n",
    "    os_is_windows,\n",
    "    os_is_macos,\n",
    "    has_sse4,\n",
    "    has_avx,\n",
    "    has_avx2,\n",
    "    has_avx512f,\n",
    "    has_vnni,\n",
    "    has_neon,\n",
    "    is_apple_m1,\n",
    "    has_intel_amx,\n",
    "    _current_target,\n",
    "    _current_cpu,\n",
    "    _triple_attr,\n",
    ")\n",
    "\n",
    "let os: StringLiteral\n",
    "if os_is_linux():\n",
    "    os = \"linux\"\n",
    "elif os_is_macos():\n",
    "    os = \"macOS\"\n",
    "else:\n",
    "    os = \"windows\"\n",
    "\n",
    "let cpu = String(_current_cpu())\n",
    "let arch = String(_triple_attr())\n",
    "\n",
    "var cpu_features = String(\"\")\n",
    "if has_sse4():\n",
    "    cpu_features += \" sse4\"\n",
    "if has_avx():\n",
    "    cpu_features += \" avx\"\n",
    "if has_avx2():\n",
    "    cpu_features += \" avx2\"\n",
    "if has_avx512f():\n",
    "    cpu_features += \" avx512f\"\n",
    "if has_vnni():\n",
    "    if has_avx512f():\n",
    "        cpu_features += \" avx512_vnni\"\n",
    "    else:\n",
    "        cpu_features += \" avx_vnni\"\n",
    "if has_intel_amx():\n",
    "    cpu_features += \" intel_amx\"\n",
    "if has_neon():\n",
    "    cpu_features += \" neon\"\n",
    "if is_apple_m1():\n",
    "    cpu_features += \" apple_m1\"\n",
    "\n",
    "if len(cpu_features) > 0:\n",
    "    cpu_features = cpu_features[1:]\n",
    "\n",
    "print(\"System Information\")\n",
    "print(\"    OS          :\", os)\n",
    "print(\"    CPU         :\", cpu)\n",
    "print(\"    Arch        :\", arch)\n",
    "print(\"    Num Cores   :\", num_cores())\n",
    "print(\"    CPU Features:\", cpu_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpmath version: 1.3.0\n",
      "NumPy version: 1.26.0\n",
      "Python version: 3.11.5 (main, Sep 11 2023, 14:07:11) [GCC 11.2.0]\n",
      "SciPy version: 1.11.3\n",
      "Tabulate version: 0.9.0\n",
      "TensorFlow version: 2.15.0\n",
      "TensorFlow Probability version: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import pkg_resources\n",
    "import sys\n",
    "\n",
    "def get_version(package):\n",
    "    \"\"\"Returns the version of a Python package.\"\"\"\n",
    "    return pkg_resources.get_distribution(package).version\n",
    "\n",
    "print(\"mpmath version:\", mp.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"SciPy version:\", scipy.__version__)\n",
    "print(\"Tabulate version:\", get_version(\"tabulate\"))\n",
    "print(\"TensorFlow version:\", get_version(\"tensorflow\"))\n",
    "print(\"TensorFlow Probability version:\", tfp.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
